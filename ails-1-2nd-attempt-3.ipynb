{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color:rgb(0,120,170)\">Artificial Intelligence in Life Sciences</h1>\n<h2 style=\"color:rgb(0,120,170)\">QSAR and model evaluation</h2>","metadata":{}},{"cell_type":"markdown","source":"<b>Authors:</b> Rumetshofer, Renz, Schimunek <br>\n<b>Date:</b> 24-03-2022\n\nThis file is part of the \"Artificial Intelligence in Life Sciences\" lecture material.\nThe following copyright statement applies to all code within this file.\n\n<b>Copyright statement:</b><br>\nThis material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational\nuse only. Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed\nor in electronic form, requires explicit prior acceptance of the authors.","metadata":{}},{"cell_type":"code","source":"!pip install rdkit\nimport os\nimport pandas as pd\nimport numpy as np\nimport copy\n\nimport rdkit\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem, DataStructs\n\nfrom rdkit import RDLogger  \nRDLogger.DisableLog('rdApp.*') \n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n\n\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(rdkit.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T20:22:28.629258Z","iopub.execute_input":"2022-08-19T20:22:28.629596Z","iopub.status.idle":"2022-08-19T20:22:37.062418Z","shell.execute_reply.started":"2022-08-19T20:22:28.629573Z","shell.execute_reply":"2022-08-19T20:22:37.061149Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color:rgb(0,120,170)\">The Tox21 dataset</h1>\n\nThe Tox21 dataset comprises roughly 13,000 compounds (12,060 training samples, 647 test samples) tested for 12 different toxicological assays (active/inactive). The label matrix contains a lot of missing labels denoted as <i>NA</i>.\n\nMore information about the dataset can be found here: https://tripod.nih.gov/tox21/challenge/\n\nHere we use a version of the dataset where the samples have been clustered and then assigned to 5 different folds.","metadata":{}},{"cell_type":"code","source":"# Preprocessed Tox21 dataset with pre-assigned clusters\ndata = pd.read_csv(\"../input/ails-first/data_train.csv\",index_col=0).reset_index(drop=True)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-08-19T18:43:39.663405Z","iopub.execute_input":"2022-08-19T18:43:39.663784Z","iopub.status.idle":"2022-08-19T18:43:39.800086Z","shell.execute_reply.started":"2022-08-19T18:43:39.663751Z","shell.execute_reply":"2022-08-19T18:43:39.799127Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"color:rgb(0,120,170)\">Data preprocessing</h2>","metadata":{}},{"cell_type":"markdown","source":"In order to use the dataset for training a model we replace the missing values with `-1`.","metadata":{}},{"cell_type":"code","source":"# Select labels, replace NaNs, convert to numpy array\ny = data[data.columns[1:]].fillna(-1)\ny = y.to_numpy()\ny.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-19T18:43:39.801469Z","iopub.execute_input":"2022-08-19T18:43:39.802733Z","iopub.status.idle":"2022-08-19T18:43:39.814652Z","shell.execute_reply.started":"2022-08-19T18:43:39.802696Z","shell.execute_reply":"2022-08-19T18:43:39.813410Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Next, we calculate Morgan fingerprints from the Smiles string for each sample.","metadata":{}},{"cell_type":"code","source":"# Initialize variables\nfp_length = 1024\nfps = np.zeros((len(data), fp_length))\n\n# Calculate Morgan fingerprints and convert to numpy array\nfor i, smiles in enumerate(tqdm(data['smiles'])):\n    mol = Chem.MolFromSmiles(smiles)\n    fp_vec = AllChem.GetMorganFingerprintAsBitVect(mol, radius=3, nBits=fp_length)\n    arr = np.zeros((1,))\n    DataStructs.ConvertToNumpyArray(fp_vec, arr)\n    fps[i] = arr","metadata":{"execution":{"iopub.status.busy":"2022-08-19T18:43:39.817506Z","iopub.execute_input":"2022-08-19T18:43:39.818438Z","iopub.status.idle":"2022-08-19T18:43:44.261938Z","shell.execute_reply.started":"2022-08-19T18:43:39.818392Z","shell.execute_reply":"2022-08-19T18:43:44.260559Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Test data Fps\ntest = pd.read_csv(\"../input/ails-first/smiles_test.csv\")\n# Initialize variables\nfp_length = 1024\nfps_test = np.zeros((len(test), fp_length))\n\n# Calculate Morgan fingerprints and convert to numpy array\nfor i, smiles in enumerate(tqdm(test['smiles'])):\n    mol = Chem.MolFromSmiles(smiles)\n    fp_vec = AllChem.GetMorganFingerprintAsBitVect(mol, radius=3, nBits=fp_length)\n    arr = np.zeros((1,))\n    DataStructs.ConvertToNumpyArray(fp_vec, arr)\n    fps_test[i] = arr","metadata":{"execution":{"iopub.status.busy":"2022-08-19T18:43:44.263729Z","iopub.execute_input":"2022-08-19T18:43:44.264606Z","iopub.status.idle":"2022-08-19T18:43:46.601484Z","shell.execute_reply.started":"2022-08-19T18:43:44.264559Z","shell.execute_reply":"2022-08-19T18:43:46.600328Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color:rgb(0,120,170)\">Train model on random split</h1>","metadata":{}},{"cell_type":"markdown","source":"We can now train a random forest model using the Morgan fingerprints as input. Here we don't use the clustering information but train the model on the standard train/test split where samples were randomly assigned to the respective set.","metadata":{}},{"cell_type":"code","source":"# Function returning the training and test sets\nfrom imblearn.ensemble import BalancedBaggingClassifier\n\ndef split_data(test_fold, X, y, folds):\n    test_mask = test_fold==folds.values\n    X_test = X[test_mask]\n    y_test = y[test_mask]\n\n    train_mask = test_fold!=folds.values\n    X_train = X[train_mask]\n    y_train = y[train_mask]\n\n    return X_train, X_test, y_train, y_test\n\n# Train a random forest model for each task on the supplied training set and return predictions for the test set\ndef train_rf(X_train, y_train, X_test):\n    seed = 1234\n    n_tasks = y_train.shape[1]\n    y_hats_proba = np.empty((X_test.shape[0], n_tasks))\n    y_hats_class = np.empty_like(y_hats_proba)\n    \n    # Train RF per task\n    for j in tqdm(range(n_tasks)):\n        rf_model = BalancedBaggingClassifier(base_estimator=RandomForestClassifier(n_estimators = 500, max_features = 'auto', max_depth = 8, criterion = 'gini'),\n                                sampling_strategy='not majority',\n                                replacement=False,\n                                random_state=42)\n       # Mask out unknown samples\n        idx = (y_train[:, j] != (0))\n        # Train model\n        rf_model.fit(X_train[idx], y_train[idx, j])\n        # Predict class probabilities (select only values for positiv class with index 1)\n        y_hats_proba[:, j] = rf_model.predict_proba(X_test)[:, 1]\n        # Predict class \n        y_hats_class[:, j] = rf_model.predict(X_test)\n    return y_hats_proba, y_hats_class ","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:12:14.900981Z","iopub.execute_input":"2022-08-19T19:12:14.901366Z","iopub.status.idle":"2022-08-19T19:12:14.913437Z","shell.execute_reply.started":"2022-08-19T19:12:14.901335Z","shell.execute_reply":"2022-08-19T19:12:14.912004Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Split data into training and test set using the random split from the dataset\nX_train, X_test, y_train, y_test = train_test_split(fps, y ,test_size=0.2)\n# Train a random forest model and get predictions for the test set\ny_hats_proba, y_hats_class = train_rf(X_train, y_train, fps_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:12:15.380373Z","iopub.execute_input":"2022-08-19T19:12:15.381103Z","iopub.status.idle":"2022-08-19T19:17:19.385340Z","shell.execute_reply.started":"2022-08-19T19:12:15.381066Z","shell.execute_reply":"2022-08-19T19:17:19.383946Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"We can look at the predicted probabilities and classes. Since the model doesn't know which values of the test set are actually measured and which are missing we get a prediction for each sample.","metadata":{}},{"cell_type":"code","source":"result = pd.DataFrame(y_hats_proba)\nresult.columns =  ['task1','task2','task3','task4','task5','task6','task7','task8','task9','task10','task11']\nresult.to_csv('result.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:17:35.574946Z","iopub.execute_input":"2022-08-19T19:17:35.575413Z","iopub.status.idle":"2022-08-19T19:17:35.696825Z","shell.execute_reply.started":"2022-08-19T19:17:35.575377Z","shell.execute_reply":"2022-08-19T19:17:35.695437Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(y_hats_class)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:11:06.866710Z","iopub.execute_input":"2022-08-19T19:11:06.867149Z","iopub.status.idle":"2022-08-19T19:11:06.897979Z","shell.execute_reply.started":"2022-08-19T19:11:06.867117Z","shell.execute_reply":"2022-08-19T19:11:06.896713Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We can also look at the predicted number of samples for each class.","metadata":{}},{"cell_type":"code","source":"unique, counts = np.unique(y_hats_class, return_counts=True)\ndict(zip(unique, counts))","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:11:10.344223Z","iopub.execute_input":"2022-08-19T19:11:10.344739Z","iopub.status.idle":"2022-08-19T19:11:10.355273Z","shell.execute_reply.started":"2022-08-19T19:11:10.344704Z","shell.execute_reply":"2022-08-19T19:11:10.353881Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color:rgb(0,120,170)\">Metrics</h1>","metadata":{}},{"cell_type":"markdown","source":"To determine the quality of the model we look at several metrics. When calculating metrics we need to remove predictions for missing values as there's no way to measure the quality of these predictions.","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"color:rgb(0,120,170)\">Confusion Matrix, Precision, Recall, F1-score</h2>","metadata":{}},{"cell_type":"markdown","source":"Lets look at these metrics (or methods) for the first task.","metadata":{}},{"cell_type":"code","source":"task = 0\n# Mask out unknown samples\nidx = (y_test[:, task] != (-1))","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:11:16.004042Z","iopub.execute_input":"2022-08-19T19:11:16.004910Z","iopub.status.idle":"2022-08-19T19:11:16.010621Z","shell.execute_reply.started":"2022-08-19T19:11:16.004832Z","shell.execute_reply":"2022-08-19T19:11:16.009548Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test[idx,task], y_hats_class[idx,task])\ncm","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:11:19.201129Z","iopub.execute_input":"2022-08-19T19:11:19.201541Z","iopub.status.idle":"2022-08-19T19:11:19.245681Z","shell.execute_reply.started":"2022-08-19T19:11:19.201509Z","shell.execute_reply":"2022-08-19T19:11:19.243925Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# True Negatives, False Positives, False Negatives, True Positives\ncm.ravel()","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:11:31.567243Z","iopub.execute_input":"2022-08-19T19:11:31.567927Z","iopub.status.idle":"2022-08-19T19:11:31.586645Z","shell.execute_reply.started":"2022-08-19T19:11:31.567890Z","shell.execute_reply":"2022-08-19T19:11:31.585225Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Precision, Recall and F1-Score","metadata":{}},{"cell_type":"markdown","source":"- The **precision** is the ratio $\\frac{TP}{TP + FP}$ where TP is the number of true positives and FP the number of false positives. The precision is intuitively the ability of the classifier to not label negative samples as positive.\n\n- The **recall** is the ratio $\\frac{TP}{TP + FN}$ where TP is the number of true positives and FN the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\n- The **F1-score** can be interpreted as a weighted harmonic mean of the precision and recall.","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test[idx,task],y_hats_class[idx,task], target_names=[\"class 0\", \"class 1\"]))","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:11:33.691150Z","iopub.execute_input":"2022-08-19T19:11:33.691874Z","iopub.status.idle":"2022-08-19T19:11:33.712758Z","shell.execute_reply.started":"2022-08-19T19:11:33.691813Z","shell.execute_reply":"2022-08-19T19:11:33.711133Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"color:rgb(0,120,170)\">Area under the ROC curve (AUC)</h2>","metadata":{}},{"cell_type":"markdown","source":"Next, we calculate the AUC for each task and the mean over all tasks.","metadata":{}},{"cell_type":"code","source":"def calc_masked_AUC_per_task(prediction, target):\n    auc_per_task = []\n    for j in range(target.shape[1]):\n        y_score = prediction[:, j]\n        y_true = target[:, j]\n        # Mask out unknown samples\n        idx = (y_true != (-1))\n        # Calculate AUC per task\n        auc_per_task.append(roc_auc_score(y_true[idx], y_score[idx]))\n    return auc_per_task","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:11:42.200777Z","iopub.execute_input":"2022-08-19T19:11:42.201208Z","iopub.status.idle":"2022-08-19T19:11:42.210526Z","shell.execute_reply.started":"2022-08-19T19:11:42.201175Z","shell.execute_reply":"2022-08-19T19:11:42.208955Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Calculate AUC per task\nauc_per_task = calc_masked_AUC_per_task(y_hats_proba, y_test)\nauc_per_task","metadata":{"execution":{"iopub.status.busy":"2022-08-19T19:11:42.746774Z","iopub.execute_input":"2022-08-19T19:11:42.748124Z","iopub.status.idle":"2022-08-19T19:11:42.780325Z","shell.execute_reply.started":"2022-08-19T19:11:42.748070Z","shell.execute_reply":"2022-08-19T19:11:42.778601Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"np.mean(auc_per_task)","metadata":{"execution":{"iopub.status.busy":"2022-08-14T08:49:12.365274Z","iopub.execute_input":"2022-08-14T08:49:12.365598Z","iopub.status.idle":"2022-08-14T08:49:12.372666Z","shell.execute_reply.started":"2022-08-14T08:49:12.365569Z","shell.execute_reply":"2022-08-14T08:49:12.371670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color:rgb(0,120,170)\">Cluster Cross-Validation</h1>","metadata":{}},{"cell_type":"markdown","source":"The previous model was trained with samples randomly assigned to the training and test sets. However, if we want to know how well our model generalizes to future data it might be a better idea to assign the training and test samples based on structural similarity. If we cluster the samples and assign all samples of some clusters to the training set and all samples of the other clusters to the test set we avoid that very similar samples are in the training and test sets.","metadata":{}},{"cell_type":"code","source":"first = pd.read_csv(\"../input/result-ails-final/result_gamble.csv\")\nfirst = first[['task1','task3','task4','task5','task9','task10','task11']]\nsecond = pd.read_csv(\"../input/result-ails-final/2nd_best.csv\")\nsecond = second[['task2','task7','task8']]\nthird = pd.read_csv(\"../input/result-ails-final/task6.csv\")\nthird = third[['task6']]\n\n\nresult = pd.concat([first, second,third], axis=1)\nresult =  result[['task1','task2','task3','task4','task5','task6','task7','task8','task9','task10','task11']]\nprint(result)\n\nresult.to_csv('result_final.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-19T20:22:49.959030Z","iopub.execute_input":"2022-08-19T20:22:49.960242Z","iopub.status.idle":"2022-08-19T20:22:50.087274Z","shell.execute_reply.started":"2022-08-19T20:22:49.960188Z","shell.execute_reply":"2022-08-19T20:22:50.086322Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}